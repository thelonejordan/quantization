# References

[LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale](https://arxiv.org/abs/2208.07339)
[LLM.int8() and Emergent Features](https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/)

[The case for 4-bit precision: k-bit Inference Scaling Laws](https://arxiv.org/abs/2212.09720)

[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)

[TimDettmers/bitsandbytes](https://github.com/TimDettmers/bitsandbytes)
[docs - bitsandbytes](https://huggingface.co/docs/bitsandbytes/v0.43.0/index)

[GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers](https://arxiv.org/abs/2210.17323)
[IST-DASLab/gptq](https://github.com/IST-DASLab/gptq)

[AutoGPTQ/AutoGPTQ](https://github.com/AutoGPTQ/AutoGPTQ)

[PB-LLM: Partially Binarized Large Language Models](https://arxiv.org/abs/2310.00034)
[hahnyuan/PB-LLM](https://github.com/hahnyuan/PB-LLM)

### HuggingFace

[A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes (blogpost)](https://huggingface.co/blog/hf-bitsandbytes-integration)
[Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA (blogpost)](https://huggingface.co/blog/4bit-transformers-bitsandbytes)
[Making LLMs lighter with AutoGPTQ and transformers (blogpost)](https://huggingface.co/blog/gptq-integration)
[Introduction to Quantization cooked in ü§ó with üíóüßë‚Äçüç≥ (community blogpost)](https://huggingface.co/blog/merve/quantization)

[Quantization - docs (transformers)](https://huggingface.co/docs/transformers/v4.39.2/quantization)
[Optimizing LLMs for Speed and Memory - docs (transformers)](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization)
[Contribute new quantization method - docs (transformers)](https://huggingface.co/docs/transformers/main/en/hf_quantizer)

[Quantization - docs (optimum)](https://huggingface.co/docs/optimum/v1.17.1/concept_guides/quantization)
[GPTQ Quantization - docs (optimum)](https://huggingface.co/docs/optimum/v1.17.1/llm_quantization/usage_guides/quantization)

[Quantization - docs (peft)](https://huggingface.co/docs/peft/main/en/developer_guides/quantization)

## Related

[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
[microsoft/LoRA](https://github.com/microsoft/LoRA)
[LoRA - conceptual guide (peft)](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)

[Adapters - conceptual guide (peft)](https://huggingface.co/docs/peft/main/en/conceptual_guides/adapter)

[AdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning](https://arxiv.org/abs/2303.10512)
[AdaLoRA](https://github.com/QingruZhang/AdaLoRA)

[peft papers](https://huggingface.co/collections/PEFT/peft-papers-6573a1a95da75f987fb873ad)
